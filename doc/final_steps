=====================
'''Final Steps'''
---------------------

** Using a local Instance, with the dbDump from CDS loaded onto it, curate: 

>    Authors List -> get the most important ones (like out of 10,000) using this http://inspirehep.net/info/hep/stats/topcites/index

         How to get a list of authors:
	     https://inspirehep.net/search?ln=en&p=&ot=100__a&rg=250&action_search=Search
	     with jrec parameter we can handle pagination.
	 ot = 100__  >> gives all the authors, not just a single, primary one

>    Keywords, Authors and Article Topic Correlations --> JSON file

>    Photographs of Physicists (if not present, then their names)

>    Introduce PyBossa somehow, somewhere, for filtering out the keywords.

>    THE LIST OF KEYWORDS used by CERN -> http://www-library.desy.de/schlagw2.html  & its usage https://invenio-demo.cern.ch/help/hacking/bibclassify-hep-taxonomy

>    Decide a Visualization and format the input accordingly.

>    Implement such an interface that makes it impressive and comprehensible for both Physicists and normal Users. (somehow linking the cause and impact of the Physics being shown)

==========================
'''Addition Information'''
--------------------------

** Primary metdata requirements are:

>    Topic
>    Author(s)
>    Abstract
>    Citations
>    Year
>    References
>    Top Cites, yearly distributed index(if possible)
>    keywords list per record (if it exists?)
>    A link to plain text downloads for all records

** THE LIST OF KEYWORDS used by CERN -> http://www-library.desy.de/schlagw2.html  & its usage https://invenio-demo.cern.ch/help/hacking/bibclassify-hep-taxonomy

** For uploading the DB dump (if any), follow these simple steps:

   # go to the parent folder which contains sub-folders containing XML files
   # Run this python script for putting all XML files in sub-folder to a common XML file:

     	 import os
	 folders = os.listdir(".")
	 for folder in folders:
	     os.chdir(folder)
	     os.system("cat * > %s.xml" % folder)
	     os.chdir("..")
   
   # Move all major XML files outside folder for getting all in 1 place for further processing
   shell> mv ?/?.xml . ; mv ??/??.xml .

   # uplaod to invenio instance
   shell> for i in `ls *.xml`; do sudo -u www-data /opt/invenio/bin/bibupload -i -r --force $i ; done

   # Run webcoll to upate on webinterface
   shell> sudo -u www-data /opt/invenio/bin/webcoll   
   
   # check task(s) failure/success through bibsched to confirm upload
   shell> sudo -u www-data bibsched

=================
'''BottleNecks''' 
-----------------

** Things to figure out:

>   way of bypassing Hourly Rate Limits 
    	-> edit conf files??


>   way of bypassing the now 200 result return limit.

  -> Edit invenio.conf and go to 

    ## CFG_WEBSEARCH_DEF_RECORDS_IN_GROUPS -- the default number of                                                                                             ## records to display per page in the search results pages.                                                                                             
    CFG_WEBSEARCH_DEF_RECORDS_IN_GROUPS = 10

    ## CFG_WEBSEARCH_MAX_RECORDS_IN_GROUPS -- in order to limit denial of                                                                                   
    ## service attacks the total number of records per group displayed as a                                                                                     ## result of a search query will be limited to this number. Only the superuser                                                                          
    ## queries will not be affected by this limit.                                      
    CFG_WEBSEARCH_MAX_RECORDS_IN_GROUPS = 200

    Voila! change the numbers! So, now map the dbDump onto local instance and change your numbers and finally just query local instance for returning larger number of records.


>   Eliminate Admin rights, so anyone could generate keywords from a document --> https://pcuds54.cern.ch/record/91/keywords (CERN internal link - not accessible outside, just in case)


>   Make an API or a script that queries the server in a way that doesnt put load on CERN's servers. (we are mapping 1M records now for God's sake! :'D )

>   Bibclassify's Keyword error investigation  --> [ AttributeError: 'BibDocFile' object has no attribute 'doctype' (bibclassify_daemon.py:356:_analyze_documents) ]
     --> use help from http://invenio-demo.cern.ch/help/admin/bibclassify-admin-guide

